{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Conv-con-LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3dyso9YVZaQ"
      },
      "source": [
        "#Segunda ronda de experimentos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jx5xXo5tVeyL"
      },
      "source": [
        "Recargamos los datos y los transformamos para la nueva representación que usaremos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrnoK-PY6qrs"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils import data\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmZEfo6F6x53"
      },
      "source": [
        "# train_dataloader = torch.load('/content/drive/MyDrive/256TrainDataloader.pth')\n",
        "# test_dataloader = torch.load('/content/drive/MyDrive/256TestDataloader.pth')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4yLPPS4D4GP"
      },
      "source": [
        "def to_categorical(y, num_classes):\n",
        "    \"\"\" 1-hot encodes a tensor \"\"\"\n",
        "    return np.eye(num_classes, dtype='uint8')[y]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ue4v10Nm6x9Q",
        "outputId": "b63d2247-2496-422d-9714-cf4d48a8ac4d"
      },
      "source": [
        "x, y = np.load('/content/drive/MyDrive/x.npy'), np.load('/content/drive/MyDrive/y.npy')\n",
        "ids_1, ids_2 = np.unique(y, return_inverse=True)\n",
        "y = to_categorical(ids_2, 10)\n",
        "y"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 0, ..., 0, 0, 0],\n",
              "       [1, 0, 0, ..., 0, 0, 0],\n",
              "       [1, 0, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 0, 1],\n",
              "       [0, 0, 0, ..., 0, 0, 1],\n",
              "       [0, 0, 0, ..., 0, 0, 1]], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhuF6cKXIYz0",
        "outputId": "ac7cfb85-736f-4c15-fb1b-647b11e4ffab"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(x,y,stratify=y,test_size = 0.2,random_state=777,shuffle=True)\n",
        "print('train_X:', X_train.shape)\n",
        "print('train_y:', y_train.shape)\n",
        "print('test_X:', X_test.shape)\n",
        "print('test_y:', y_test.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_X: (2400, 256, 256)\n",
            "train_y: (2400, 10)\n",
            "test_X: (600, 256, 256)\n",
            "test_y: (600, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTIDqbIqIdYE",
        "outputId": "34a06d47-b017-45e9-80cd-2a3ddc5a9bae"
      },
      "source": [
        "#pasando a tensores\n",
        "tensor_x_train = torch.Tensor(X_train) \n",
        "tensor_y_train = torch.Tensor(y_train)\n",
        "tensor_x_train = torch.unsqueeze(tensor_x_train, 1)\n",
        "print(\"tensor_x_train.shape: \", tensor_x_train.shape)\n",
        "print(\"tensor_y_train.shape: \", tensor_y_train.shape)\n",
        "\n",
        "train_set = data.TensorDataset(tensor_x_train,tensor_y_train)\n",
        "train_dataloader = data.DataLoader(train_set,batch_size=32, shuffle=True)\n",
        "\n",
        "# Creating a dataloader for test data\n",
        "tensor_x_test = torch.Tensor(X_test)\n",
        "tensor_y_test = torch.Tensor(y_test)\n",
        "tensor_x_test = torch.unsqueeze(tensor_x_test, 1)\n",
        "print(\"tensor_x_test.shape: \", tensor_x_test.shape)\n",
        "print(\"tensor_y_test.shape: \", tensor_y_test.shape)\n",
        "test_set = data.TensorDataset(tensor_x_test, tensor_y_test)\n",
        "test_dataloader = data.DataLoader(test_set, batch_size=32, shuffle=False)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor_x_train.shape:  torch.Size([2400, 1, 256, 256])\n",
            "tensor_y_train.shape:  torch.Size([2400, 10])\n",
            "tensor_x_test.shape:  torch.Size([600, 1, 256, 256])\n",
            "tensor_y_test.shape:  torch.Size([600, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vgMkPhLUw1h"
      },
      "source": [
        "##Modelo de Conv1d y LTSM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5t-KwlGWU04U"
      },
      "source": [
        "Aquí se llevó a cabo un proseso muy largo de experimentación y prueba-error. Incluso con la representación escogida de audio en MFCC complicaba el espacio de hipótesis, y varias arquitecturas convolucionales de 2 dimensiones fallaban en encontrar una representación significante de los datos, estancandose con toda combinación de optimizadores y learning-rates en un Accuracy de 30.\n",
        "\n",
        "Por este motivo se experimentó la representación en convoluciones unidimensionales ya que los MFCC's se basan en ventanas cortas de tiempo tomadas secuencialmente, por lo que podemos entender que la representación del audi es comprendida en vectores unidimensionales.\n",
        "\n",
        "Para ingresarlo primero transformaremos la imagen a un arreglo continuo/secuencial en lugar de trabajarlo como una imagen bidimensional. Como esto representa una serie de tiempo de imágenes hace sentido utilizar una LSTM para tratar de predecir como ocurren estos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3bW5djpX7MT"
      },
      "source": [
        "Bengio y Xavier recomiedan para convoluciones usar una [xavier uniform](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)\n",
        "\n",
        "También consulté de referencia este link:\n",
        "*   https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79\n",
        "\n",
        "Los primeros resultados con una convolución2d presentaba muchos problemas con la velocidad a la que mejoraba, por lo que decidí utilizarlas en este segundo experimento.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYs-RN4B6yAM"
      },
      "source": [
        "class CRNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CRNN, self).__init__()\n",
        "    \n",
        "    self.conv1 = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=5)\n",
        "    torch.nn.init.xavier_uniform_(self.conv1.weight)\n",
        "    self.bn1 = nn.BatchNorm1d(32)\n",
        "    self.drop1  = nn.Dropout(0.5)\n",
        "\n",
        "    self.conv2 = nn.Conv1d(in_channels=32, out_channels=40, kernel_size=5)\n",
        "    torch.nn.init.xavier_uniform_(self.conv2.weight)\n",
        "    self.bn2 = nn.BatchNorm1d(40)\n",
        "    self.drop2  = nn.Dropout(0.5)\n",
        "\n",
        "    self.conv3 = nn.Conv1d(in_channels=40, out_channels=52, kernel_size=5)\n",
        "    torch.nn.init.xavier_uniform_(self.conv3.weight)\n",
        "    self.bn3 = nn.BatchNorm1d(52)\n",
        "    self.drop3  = nn.Dropout(0.5)\n",
        "\n",
        "    self.rnn = nn.LSTM(10236, 96, 1)\n",
        "    self.drop4  = nn.Dropout(0.5)\n",
        "\n",
        "    self.fc1    = nn.Linear(in_features=4992, out_features=64)\n",
        "    self.drop5  = nn.Dropout(0.5)\n",
        "    self.fc2    = nn.Linear(in_features=64,  out_features=8)\n",
        "\n",
        "  def forward(self, x):\n",
        "    #aplanamos las imagenes\n",
        "    x = x.view(-1,1, self.flat_images(x))\n",
        "\n",
        "    x = F.max_pool1d(F.relu(self.bn1(self.conv1(x))), 2)\n",
        "    x   = self.drop1(x)\n",
        "    x = F.max_pool1d(F.relu(self.bn2(self.conv2(x))), 2)\n",
        "    x   = self.drop2(x)\n",
        "    x = F.max_pool1d(F.relu(self.bn3(self.conv3(x))), 2)\n",
        "    x   = self.drop3(x)\n",
        "\n",
        "    x, (hn,cn) = self.rnn(x)\n",
        "    x = self.drop4(x)\n",
        "\n",
        "    x = x.view(-1,self.flat_images(x))\n",
        "    x = self.fc1(x)\n",
        "    x   = self.drop5(x)\n",
        "    x = self.fc2(x)\n",
        "    return x\n",
        "\n",
        "  def flat_images(self, x):\n",
        "      size = x.size()[1:] \n",
        "      num_features = 1\n",
        "      for s in size:\n",
        "          num_features *= s\n",
        "      return num_features\n",
        "    \n",
        "  def train(self,epochs,data_loader,criterion,optimizer,cuda=False): \n",
        "    error = np.zeros(epochs)\n",
        "#         since = time.time()\n",
        "    best_acc = 0.0\n",
        "    device = torch.device(\"cuda:0\") if (cuda == True) else torch.device(\"cpu:0\")\n",
        "    optimizer.zero_grad()\n",
        "    for epoch in range(epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, epochs - 1))\n",
        "        print('----------')\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = self(inputs)\n",
        "            loss = criterion(outputs,labels)\n",
        "            _, preds = torch.max(outputs,1)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()*inputs.size(0)\n",
        "            running_corrects += torch.sum(preds== torch.argmax(labels.data))\n",
        "        epoch_loss = running_loss / len(data_loader)\n",
        "        epoch_acc = running_corrects.double() / len(data_loader)\n",
        "        print('Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
        "        error[epoch] = epoch_loss\n",
        "        if(epoch_acc > best_acc):\n",
        "            best_acc = epoch_acc\n",
        "            print('----------')\n",
        "#         time_elapsed = time.time() - since\n",
        "#         print('Entrenamiento completado {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "    steps = np.arange(epochs)\n",
        "    plt.plot(steps, error, label='Error entrenamiento')\n",
        "    plt.legend()\n",
        "    plt.title('Error')\n",
        "        "
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QxNxUAst6yDI",
        "outputId": "6c4d490c-1d46-4c16-f379-bc6a311b808e"
      },
      "source": [
        "music_net=CNN()\n",
        "#Define el criterio que usaras para evaluar a la red y un optimizador.\n",
        "#Y define el preprosesamiento necesario en caso de requerir usar cuda.\n",
        "\n",
        "# optimizer = torch.optim.SGD(music_net.parameters(), lr= 0.01, momentum=0.5)\n",
        "# optimizer = torch.optim.Adam(music_net.parameters(),weight_decay=0.01)\n",
        "optimizer = torch.optim.RMSprop(music_net.parameters(), lr=1e-5)\n",
        "# criterio = nn.CrossEntropyLoss()\n",
        "criterio = nn.BCELoss()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "music_net = music_net.to(device)\n",
        "\n",
        "\n",
        "#Entrenamos la red durante 500 pasos\n",
        "music_net.train(500,train_dataloader,criterio,optimizer,cuda=True) "
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/499\n",
            "----------\n",
            "Loss: 9.9930 Acc: 3.1600\n",
            "----------\n",
            "Epoch 1/499\n",
            "----------\n",
            "Loss: 9.2362 Acc: 3.6400\n",
            "----------\n",
            "Epoch 2/499\n",
            "----------\n",
            "Loss: 8.7102 Acc: 3.1733\n",
            "Epoch 3/499\n",
            "----------\n",
            "Loss: 8.3531 Acc: 3.2933\n",
            "Epoch 4/499\n",
            "----------\n",
            "Loss: 7.9608 Acc: 3.1733\n",
            "Epoch 5/499\n",
            "----------\n",
            "Loss: 7.8323 Acc: 3.2533\n",
            "Epoch 6/499\n",
            "----------\n",
            "Loss: 7.5303 Acc: 3.4000\n",
            "Epoch 7/499\n",
            "----------\n",
            "Loss: 7.4271 Acc: 3.1600\n",
            "Epoch 8/499\n",
            "----------\n",
            "Loss: 7.1618 Acc: 3.5467\n",
            "Epoch 9/499\n",
            "----------\n",
            "Loss: 6.9144 Acc: 3.3467\n",
            "Epoch 10/499\n",
            "----------\n",
            "Loss: 6.7439 Acc: 3.8000\n",
            "----------\n",
            "Epoch 11/499\n",
            "----------\n",
            "Loss: 6.5587 Acc: 3.3867\n",
            "Epoch 12/499\n",
            "----------\n",
            "Loss: 6.4154 Acc: 3.4000\n",
            "Epoch 13/499\n",
            "----------\n",
            "Loss: 6.2839 Acc: 3.5600\n",
            "Epoch 14/499\n",
            "----------\n",
            "Loss: 6.0383 Acc: 3.1467\n",
            "Epoch 15/499\n",
            "----------\n",
            "Loss: 5.9554 Acc: 3.4000\n",
            "Epoch 16/499\n",
            "----------\n",
            "Loss: 5.7699 Acc: 4.0000\n",
            "----------\n",
            "Epoch 17/499\n",
            "----------\n",
            "Loss: 5.5984 Acc: 3.6533\n",
            "Epoch 18/499\n",
            "----------\n",
            "Loss: 5.5149 Acc: 3.5467\n",
            "Epoch 19/499\n",
            "----------\n",
            "Loss: 5.2931 Acc: 3.3867\n",
            "Epoch 20/499\n",
            "----------\n",
            "Loss: 5.2100 Acc: 3.8800\n",
            "Epoch 21/499\n",
            "----------\n",
            "Loss: 4.9960 Acc: 3.7200\n",
            "Epoch 22/499\n",
            "----------\n",
            "Loss: 4.9081 Acc: 3.7467\n",
            "Epoch 23/499\n",
            "----------\n",
            "Loss: 4.6996 Acc: 3.6933\n",
            "Epoch 24/499\n",
            "----------\n",
            "Loss: 4.5078 Acc: 3.6667\n",
            "Epoch 25/499\n",
            "----------\n",
            "Loss: 4.4282 Acc: 3.7333\n",
            "Epoch 26/499\n",
            "----------\n",
            "Loss: 4.3714 Acc: 3.5200\n",
            "Epoch 27/499\n",
            "----------\n",
            "Loss: 4.2421 Acc: 3.8000\n",
            "Epoch 28/499\n",
            "----------\n",
            "Loss: 4.0555 Acc: 3.3733\n",
            "Epoch 29/499\n",
            "----------\n",
            "Loss: 3.8744 Acc: 3.9467\n",
            "Epoch 30/499\n",
            "----------\n",
            "Loss: 3.8391 Acc: 4.1600\n",
            "----------\n",
            "Epoch 31/499\n",
            "----------\n",
            "Loss: 3.7543 Acc: 3.8800\n",
            "Epoch 32/499\n",
            "----------\n",
            "Loss: 3.6260 Acc: 3.8000\n",
            "Epoch 33/499\n",
            "----------\n",
            "Loss: 3.4337 Acc: 3.7067\n",
            "Epoch 34/499\n",
            "----------\n",
            "Loss: 3.4889 Acc: 4.1467\n",
            "Epoch 35/499\n",
            "----------\n",
            "Loss: 3.1681 Acc: 3.7600\n",
            "Epoch 36/499\n",
            "----------\n",
            "Loss: 3.1652 Acc: 3.8667\n",
            "Epoch 37/499\n",
            "----------\n",
            "Loss: 2.9615 Acc: 4.0400\n",
            "Epoch 38/499\n",
            "----------\n",
            "Loss: 2.9542 Acc: 3.8933\n",
            "Epoch 39/499\n",
            "----------\n",
            "Loss: 2.7908 Acc: 4.0267\n",
            "Epoch 40/499\n",
            "----------\n",
            "Loss: 2.7147 Acc: 3.8533\n",
            "Epoch 41/499\n",
            "----------\n",
            "Loss: 2.5634 Acc: 3.9733\n",
            "Epoch 42/499\n",
            "----------\n",
            "Loss: 2.4140 Acc: 3.8400\n",
            "Epoch 43/499\n",
            "----------\n",
            "Loss: 2.4321 Acc: 3.5600\n",
            "Epoch 44/499\n",
            "----------\n",
            "Loss: 2.2141 Acc: 4.0667\n",
            "Epoch 45/499\n",
            "----------\n",
            "Loss: 2.2286 Acc: 3.9067\n",
            "Epoch 46/499\n",
            "----------\n",
            "Loss: 2.0233 Acc: 3.8533\n",
            "Epoch 47/499\n",
            "----------\n",
            "Loss: 1.9945 Acc: 4.0267\n",
            "Epoch 48/499\n",
            "----------\n",
            "Loss: 1.8350 Acc: 3.7733\n",
            "Epoch 49/499\n",
            "----------\n",
            "Loss: 1.8022 Acc: 3.8400\n",
            "Epoch 50/499\n",
            "----------\n",
            "Loss: 1.7092 Acc: 3.5333\n",
            "Epoch 51/499\n",
            "----------\n",
            "Loss: 1.6089 Acc: 4.0933\n",
            "Epoch 52/499\n",
            "----------\n",
            "Loss: 1.4468 Acc: 3.9600\n",
            "Epoch 53/499\n",
            "----------\n",
            "Loss: 1.4690 Acc: 4.3867\n",
            "----------\n",
            "Epoch 54/499\n",
            "----------\n",
            "Loss: 1.3350 Acc: 3.9067\n",
            "Epoch 55/499\n",
            "----------\n",
            "Loss: 1.2390 Acc: 3.8667\n",
            "Epoch 56/499\n",
            "----------\n",
            "Loss: 1.2740 Acc: 4.2267\n",
            "Epoch 57/499\n",
            "----------\n",
            "Loss: 1.2344 Acc: 3.8400\n",
            "Epoch 58/499\n",
            "----------\n",
            "Loss: 0.9598 Acc: 4.3733\n",
            "Epoch 59/499\n",
            "----------\n",
            "Loss: 1.0075 Acc: 4.2400\n",
            "Epoch 60/499\n",
            "----------\n",
            "Loss: 0.9347 Acc: 4.1600\n",
            "Epoch 61/499\n",
            "----------\n",
            "Loss: 0.8893 Acc: 4.2400\n",
            "Epoch 62/499\n",
            "----------\n",
            "Loss: 0.7303 Acc: 4.5067\n",
            "----------\n",
            "Epoch 63/499\n",
            "----------\n",
            "Loss: 0.7445 Acc: 3.8000\n",
            "Epoch 64/499\n",
            "----------\n",
            "Loss: 0.7452 Acc: 4.2667\n",
            "Epoch 65/499\n",
            "----------\n",
            "Loss: 0.6880 Acc: 4.2133\n",
            "Epoch 66/499\n",
            "----------\n",
            "Loss: 0.5592 Acc: 4.1067\n",
            "Epoch 67/499\n",
            "----------\n",
            "Loss: 0.6163 Acc: 3.6533\n",
            "Epoch 68/499\n",
            "----------\n",
            "Loss: 0.6361 Acc: 3.8533\n",
            "Epoch 69/499\n",
            "----------\n",
            "Loss: 0.4882 Acc: 4.1467\n",
            "Epoch 70/499\n",
            "----------\n",
            "Loss: 0.4833 Acc: 4.0533\n",
            "Epoch 71/499\n",
            "----------\n",
            "Loss: 0.5338 Acc: 4.1867\n",
            "Epoch 72/499\n",
            "----------\n",
            "Loss: 0.4290 Acc: 4.0533\n",
            "Epoch 73/499\n",
            "----------\n",
            "Loss: 0.3752 Acc: 4.0267\n",
            "Epoch 74/499\n",
            "----------\n",
            "Loss: 0.5336 Acc: 4.4133\n",
            "Epoch 75/499\n",
            "----------\n",
            "Loss: 0.3019 Acc: 3.8800\n",
            "Epoch 76/499\n",
            "----------\n",
            "Loss: 0.3973 Acc: 4.0133\n",
            "Epoch 77/499\n",
            "----------\n",
            "Loss: 0.3338 Acc: 4.2400\n",
            "Epoch 78/499\n",
            "----------\n",
            "Loss: 0.3526 Acc: 4.0400\n",
            "Epoch 79/499\n",
            "----------\n",
            "Loss: 0.3561 Acc: 4.0933\n",
            "Epoch 80/499\n",
            "----------\n",
            "Loss: 0.2999 Acc: 4.2133\n",
            "Epoch 81/499\n",
            "----------\n",
            "Loss: 0.2798 Acc: 4.0267\n",
            "Epoch 82/499\n",
            "----------\n",
            "Loss: 0.3716 Acc: 3.9067\n",
            "Epoch 83/499\n",
            "----------\n",
            "Loss: 0.2217 Acc: 4.0267\n",
            "Epoch 84/499\n",
            "----------\n",
            "Loss: 0.1972 Acc: 4.3867\n",
            "Epoch 85/499\n",
            "----------\n",
            "Loss: 0.3263 Acc: 4.0133\n",
            "Epoch 86/499\n",
            "----------\n",
            "Loss: 0.1389 Acc: 4.1733\n",
            "Epoch 87/499\n",
            "----------\n",
            "Loss: 0.2162 Acc: 4.0400\n",
            "Epoch 88/499\n",
            "----------\n",
            "Loss: 0.1841 Acc: 3.8267\n",
            "Epoch 89/499\n",
            "----------\n",
            "Loss: 0.2404 Acc: 4.0800\n",
            "Epoch 90/499\n",
            "----------\n",
            "Loss: 0.3253 Acc: 4.2667\n",
            "Epoch 91/499\n",
            "----------\n",
            "Loss: 0.1804 Acc: 4.5200\n",
            "----------\n",
            "Epoch 92/499\n",
            "----------\n",
            "Loss: 0.1610 Acc: 4.1333\n",
            "Epoch 93/499\n",
            "----------\n",
            "Loss: 0.1334 Acc: 4.0133\n",
            "Epoch 94/499\n",
            "----------\n",
            "Loss: 0.1758 Acc: 3.8533\n",
            "Epoch 95/499\n",
            "----------\n",
            "Loss: 0.2769 Acc: 4.2533\n",
            "Epoch 96/499\n",
            "----------\n",
            "Loss: 0.1913 Acc: 4.2267\n",
            "Epoch 97/499\n",
            "----------\n",
            "Loss: 0.1223 Acc: 4.2667\n",
            "Epoch 98/499\n",
            "----------\n",
            "Loss: 0.2294 Acc: 4.4533\n",
            "Epoch 99/499\n",
            "----------\n",
            "Loss: 0.1159 Acc: 4.0267\n",
            "Epoch 100/499\n",
            "----------\n",
            "Loss: 0.2009 Acc: 4.2400\n",
            "Epoch 101/499\n",
            "----------\n",
            "Loss: 0.1192 Acc: 3.7600\n",
            "Epoch 102/499\n",
            "----------\n",
            "Loss: 0.1303 Acc: 4.5333\n",
            "----------\n",
            "Epoch 103/499\n",
            "----------\n",
            "Loss: 0.1314 Acc: 4.1867\n",
            "Epoch 104/499\n",
            "----------\n",
            "Loss: 0.1800 Acc: 3.9867\n",
            "Epoch 105/499\n",
            "----------\n",
            "Loss: 0.1019 Acc: 4.2400\n",
            "Epoch 106/499\n",
            "----------\n",
            "Loss: 0.3906 Acc: 4.2933\n",
            "Epoch 107/499\n",
            "----------\n",
            "Loss: 0.0853 Acc: 4.6667\n",
            "----------\n",
            "Epoch 108/499\n",
            "----------\n",
            "Loss: 0.0898 Acc: 3.9600\n",
            "Epoch 109/499\n",
            "----------\n",
            "Loss: 0.1163 Acc: 3.7467\n",
            "Epoch 110/499\n",
            "----------\n",
            "Loss: 0.2845 Acc: 4.1467\n",
            "Epoch 111/499\n",
            "----------\n",
            "Loss: 0.0780 Acc: 4.1467\n",
            "Epoch 112/499\n",
            "----------\n",
            "Loss: 0.1888 Acc: 4.0667\n",
            "Epoch 113/499\n",
            "----------\n",
            "Loss: 0.0990 Acc: 4.0400\n",
            "Epoch 114/499\n",
            "----------\n",
            "Loss: 0.0711 Acc: 4.0667\n",
            "Epoch 115/499\n",
            "----------\n",
            "Loss: 0.2628 Acc: 4.1600\n",
            "Epoch 116/499\n",
            "----------\n",
            "Loss: 0.1421 Acc: 4.0933\n",
            "Epoch 117/499\n",
            "----------\n",
            "Loss: 0.2304 Acc: 3.8000\n",
            "Epoch 118/499\n",
            "----------\n",
            "Loss: 0.0995 Acc: 4.1733\n",
            "Epoch 119/499\n",
            "----------\n",
            "Loss: 0.0867 Acc: 4.0267\n",
            "Epoch 120/499\n",
            "----------\n",
            "Loss: 0.1188 Acc: 3.8533\n",
            "Epoch 121/499\n",
            "----------\n",
            "Loss: 0.1213 Acc: 4.1333\n",
            "Epoch 122/499\n",
            "----------\n",
            "Loss: 0.0706 Acc: 4.1200\n",
            "Epoch 123/499\n",
            "----------\n",
            "Loss: 0.2953 Acc: 4.0400\n",
            "Epoch 124/499\n",
            "----------\n",
            "Loss: 0.0774 Acc: 4.5067\n",
            "Epoch 125/499\n",
            "----------\n",
            "Loss: 0.0766 Acc: 4.1600\n",
            "Epoch 126/499\n",
            "----------\n",
            "Loss: 0.1745 Acc: 3.6667\n",
            "Epoch 127/499\n",
            "----------\n",
            "Loss: 0.0773 Acc: 4.3467\n",
            "Epoch 128/499\n",
            "----------\n",
            "Loss: 0.0803 Acc: 4.0933\n",
            "Epoch 129/499\n",
            "----------\n",
            "Loss: 0.3119 Acc: 4.1867\n",
            "Epoch 130/499\n",
            "----------\n",
            "Loss: 0.0479 Acc: 3.8267\n",
            "Epoch 131/499\n",
            "----------\n",
            "Loss: 0.0931 Acc: 4.4400\n",
            "Epoch 132/499\n",
            "----------\n",
            "Loss: 0.0443 Acc: 4.2400\n",
            "Epoch 133/499\n",
            "----------\n",
            "Loss: 0.1152 Acc: 4.3733\n",
            "Epoch 134/499\n",
            "----------\n",
            "Loss: 0.2149 Acc: 4.2400\n",
            "Epoch 135/499\n",
            "----------\n",
            "Loss: 0.1304 Acc: 4.1467\n",
            "Epoch 136/499\n",
            "----------\n",
            "Loss: 0.0459 Acc: 3.8267\n",
            "Epoch 137/499\n",
            "----------\n",
            "Loss: 0.0626 Acc: 3.7867\n",
            "Epoch 138/499\n",
            "----------\n",
            "Loss: 0.2710 Acc: 3.8667\n",
            "Epoch 139/499\n",
            "----------\n",
            "Loss: 0.0604 Acc: 4.2267\n",
            "Epoch 140/499\n",
            "----------\n",
            "Loss: 0.0816 Acc: 4.3067\n",
            "Epoch 141/499\n",
            "----------\n",
            "Loss: 0.0708 Acc: 4.1333\n",
            "Epoch 142/499\n",
            "----------\n",
            "Loss: 0.1509 Acc: 4.0667\n",
            "Epoch 143/499\n",
            "----------\n",
            "Loss: 0.0347 Acc: 4.2267\n",
            "Epoch 144/499\n",
            "----------\n",
            "Loss: 0.1193 Acc: 3.7600\n",
            "Epoch 145/499\n",
            "----------\n",
            "Loss: 0.0242 Acc: 4.1333\n",
            "Epoch 146/499\n",
            "----------\n",
            "Loss: 0.2901 Acc: 3.7867\n",
            "Epoch 147/499\n",
            "----------\n",
            "Loss: 0.0651 Acc: 4.2933\n",
            "Epoch 148/499\n",
            "----------\n",
            "Loss: 0.0594 Acc: 4.1333\n",
            "Epoch 149/499\n",
            "----------\n",
            "Loss: 0.0850 Acc: 4.5067\n",
            "Epoch 150/499\n",
            "----------\n",
            "Loss: 0.1092 Acc: 3.8400\n",
            "Epoch 151/499\n",
            "----------\n",
            "Loss: 0.0982 Acc: 4.0267\n",
            "Epoch 152/499\n",
            "----------\n",
            "Loss: 0.0446 Acc: 4.5200\n",
            "Epoch 153/499\n",
            "----------\n",
            "Loss: 0.1262 Acc: 4.3867\n",
            "Epoch 154/499\n",
            "----------\n",
            "Loss: 0.1040 Acc: 4.1067\n",
            "Epoch 155/499\n",
            "----------\n",
            "Loss: 0.0534 Acc: 4.1067\n",
            "Epoch 156/499\n",
            "----------\n",
            "Loss: 0.1247 Acc: 3.7333\n",
            "Epoch 157/499\n",
            "----------\n",
            "Loss: 0.3172 Acc: 4.3067\n",
            "Epoch 158/499\n",
            "----------\n",
            "Loss: 0.0388 Acc: 4.1467\n",
            "Epoch 159/499\n",
            "----------\n",
            "Loss: 0.0219 Acc: 4.4133\n",
            "Epoch 160/499\n",
            "----------\n",
            "Loss: 0.0822 Acc: 4.2267\n",
            "Epoch 161/499\n",
            "----------\n",
            "Loss: 0.1169 Acc: 4.0933\n",
            "Epoch 162/499\n",
            "----------\n",
            "Loss: 0.0158 Acc: 3.8533\n",
            "Epoch 163/499\n",
            "----------\n",
            "Loss: 0.2647 Acc: 4.4400\n",
            "Epoch 164/499\n",
            "----------\n",
            "Loss: 0.0521 Acc: 4.3067\n",
            "Epoch 165/499\n",
            "----------\n",
            "Loss: 0.0575 Acc: 3.9867\n",
            "Epoch 166/499\n",
            "----------\n",
            "Loss: 0.1359 Acc: 4.1467\n",
            "Epoch 167/499\n",
            "----------\n",
            "Loss: 0.0770 Acc: 3.9600\n",
            "Epoch 168/499\n",
            "----------\n",
            "Loss: 0.0711 Acc: 4.2000\n",
            "Epoch 169/499\n",
            "----------\n",
            "Loss: 0.0355 Acc: 4.2667\n",
            "Epoch 170/499\n",
            "----------\n",
            "Loss: 0.0585 Acc: 3.9733\n",
            "Epoch 171/499\n",
            "----------\n",
            "Loss: 0.0505 Acc: 4.1067\n",
            "Epoch 172/499\n",
            "----------\n",
            "Loss: 0.4052 Acc: 3.7733\n",
            "Epoch 173/499\n",
            "----------\n",
            "Loss: 0.0407 Acc: 4.1867\n",
            "Epoch 174/499\n",
            "----------\n",
            "Loss: 0.0425 Acc: 3.9867\n",
            "Epoch 175/499\n",
            "----------\n",
            "Loss: 0.0683 Acc: 4.4133\n",
            "Epoch 176/499\n",
            "----------\n",
            "Loss: 0.0718 Acc: 3.9467\n",
            "Epoch 177/499\n",
            "----------\n",
            "Loss: 0.0510 Acc: 3.9733\n",
            "Epoch 178/499\n",
            "----------\n",
            "Loss: 0.1627 Acc: 4.0000\n",
            "Epoch 179/499\n",
            "----------\n",
            "Loss: 0.0384 Acc: 4.0400\n",
            "Epoch 180/499\n",
            "----------\n",
            "Loss: 0.1913 Acc: 4.0133\n",
            "Epoch 181/499\n",
            "----------\n",
            "Loss: 0.1402 Acc: 4.3600\n",
            "Epoch 182/499\n",
            "----------\n",
            "Loss: 0.0373 Acc: 4.2400\n",
            "Epoch 183/499\n",
            "----------\n",
            "Loss: 0.1048 Acc: 4.0933\n",
            "Epoch 184/499\n",
            "----------\n",
            "Loss: 0.0354 Acc: 4.0533\n",
            "Epoch 185/499\n",
            "----------\n",
            "Loss: 0.1106 Acc: 4.0667\n",
            "Epoch 186/499\n",
            "----------\n",
            "Loss: 0.1821 Acc: 3.9200\n",
            "Epoch 187/499\n",
            "----------\n",
            "Loss: 0.0451 Acc: 4.3733\n",
            "Epoch 188/499\n",
            "----------\n",
            "Loss: 0.0485 Acc: 4.3867\n",
            "Epoch 189/499\n",
            "----------\n",
            "Loss: 0.1444 Acc: 4.2667\n",
            "Epoch 190/499\n",
            "----------\n",
            "Loss: 0.0399 Acc: 4.0667\n",
            "Epoch 191/499\n",
            "----------\n",
            "Loss: 0.0396 Acc: 4.1333\n",
            "Epoch 192/499\n",
            "----------\n",
            "Loss: 0.2220 Acc: 3.7467\n",
            "Epoch 193/499\n",
            "----------\n",
            "Loss: 0.3197 Acc: 4.0933\n",
            "Epoch 194/499\n",
            "----------\n",
            "Loss: 0.0291 Acc: 4.4533\n",
            "Epoch 195/499\n",
            "----------\n",
            "Loss: 0.0653 Acc: 4.2800\n",
            "Epoch 196/499\n",
            "----------\n",
            "Loss: 0.0909 Acc: 4.0933\n",
            "Epoch 197/499\n",
            "----------\n",
            "Loss: 0.0265 Acc: 3.9733\n",
            "Epoch 198/499\n",
            "----------\n",
            "Loss: 0.1168 Acc: 4.3067\n",
            "Epoch 199/499\n",
            "----------\n",
            "Loss: 0.1237 Acc: 3.9867\n",
            "Epoch 200/499\n",
            "----------\n",
            "Loss: 0.0833 Acc: 4.0000\n",
            "Epoch 201/499\n",
            "----------\n",
            "Loss: 0.0684 Acc: 3.9733\n",
            "Epoch 202/499\n",
            "----------\n",
            "Loss: 0.0253 Acc: 4.2267\n",
            "Epoch 203/499\n",
            "----------\n",
            "Loss: 0.1495 Acc: 4.5600\n",
            "Epoch 204/499\n",
            "----------\n",
            "Loss: 0.0638 Acc: 4.1067\n",
            "Epoch 205/499\n",
            "----------\n",
            "Loss: 0.0364 Acc: 4.2533\n",
            "Epoch 206/499\n",
            "----------\n",
            "Loss: 0.0754 Acc: 4.2267\n",
            "Epoch 207/499\n",
            "----------\n",
            "Loss: 0.0859 Acc: 3.9200\n",
            "Epoch 208/499\n",
            "----------\n",
            "Loss: 0.0627 Acc: 3.9733\n",
            "Epoch 209/499\n",
            "----------\n",
            "Loss: 0.0746 Acc: 3.9333\n",
            "Epoch 210/499\n",
            "----------\n",
            "Loss: 0.0270 Acc: 4.5333\n",
            "Epoch 211/499\n",
            "----------\n",
            "Loss: 0.1478 Acc: 3.8933\n",
            "Epoch 212/499\n",
            "----------\n",
            "Loss: 0.0383 Acc: 3.8667\n",
            "Epoch 213/499\n",
            "----------\n",
            "Loss: 0.0923 Acc: 3.8533\n",
            "Epoch 214/499\n",
            "----------\n",
            "Loss: 0.0365 Acc: 4.3067\n",
            "Epoch 215/499\n",
            "----------\n",
            "Loss: 0.1023 Acc: 4.2000\n",
            "Epoch 216/499\n",
            "----------\n",
            "Loss: 0.0293 Acc: 3.7067\n",
            "Epoch 217/499\n",
            "----------\n",
            "Loss: 0.0062 Acc: 3.4800\n",
            "Epoch 218/499\n",
            "----------\n",
            "Loss: 0.3946 Acc: 3.7867\n",
            "Epoch 219/499\n",
            "----------\n",
            "Loss: 0.0286 Acc: 3.8533\n",
            "Epoch 220/499\n",
            "----------\n",
            "Loss: 0.0219 Acc: 3.8667\n",
            "Epoch 221/499\n",
            "----------\n",
            "Loss: 0.0131 Acc: 4.1067\n",
            "Epoch 222/499\n",
            "----------\n",
            "Loss: 0.0312 Acc: 4.2000\n",
            "Epoch 223/499\n",
            "----------\n",
            "Loss: 0.2451 Acc: 4.0533\n",
            "Epoch 224/499\n",
            "----------\n",
            "Loss: 0.0295 Acc: 4.1867\n",
            "Epoch 225/499\n",
            "----------\n",
            "Loss: 0.0911 Acc: 4.2533\n",
            "Epoch 226/499\n",
            "----------\n",
            "Loss: 0.0547 Acc: 3.8267\n",
            "Epoch 227/499\n",
            "----------\n",
            "Loss: 0.2480 Acc: 4.5200\n",
            "Epoch 228/499\n",
            "----------\n",
            "Loss: 0.0210 Acc: 4.3067\n",
            "Epoch 229/499\n",
            "----------\n",
            "Loss: 0.0295 Acc: 3.9867\n",
            "Epoch 230/499\n",
            "----------\n",
            "Loss: 0.0253 Acc: 3.6000\n",
            "Epoch 231/499\n",
            "----------\n",
            "Loss: 0.1472 Acc: 4.0533\n",
            "Epoch 232/499\n",
            "----------\n",
            "Loss: 0.0263 Acc: 4.0533\n",
            "Epoch 233/499\n",
            "----------\n",
            "Loss: 0.1123 Acc: 4.2800\n",
            "Epoch 234/499\n",
            "----------\n",
            "Loss: 0.0589 Acc: 3.8000\n",
            "Epoch 235/499\n",
            "----------\n",
            "Loss: 0.0227 Acc: 4.0000\n",
            "Epoch 236/499\n",
            "----------\n",
            "Loss: 0.0249 Acc: 4.1600\n",
            "Epoch 237/499\n",
            "----------\n",
            "Loss: 0.1177 Acc: 4.2267\n",
            "Epoch 238/499\n",
            "----------\n",
            "Loss: 0.0324 Acc: 3.9600\n",
            "Epoch 239/499\n",
            "----------\n",
            "Loss: 0.1391 Acc: 4.2267\n",
            "Epoch 240/499\n",
            "----------\n",
            "Loss: 0.0193 Acc: 4.0667\n",
            "Epoch 241/499\n",
            "----------\n",
            "Loss: 0.1595 Acc: 4.0133\n",
            "Epoch 242/499\n",
            "----------\n",
            "Loss: 0.0753 Acc: 4.0667\n",
            "Epoch 243/499\n",
            "----------\n",
            "Loss: 0.0203 Acc: 4.2800\n",
            "Epoch 244/499\n",
            "----------\n",
            "Loss: 0.0319 Acc: 3.8533\n",
            "Epoch 245/499\n",
            "----------\n",
            "Loss: 0.1550 Acc: 4.4267\n",
            "Epoch 246/499\n",
            "----------\n",
            "Loss: 0.0455 Acc: 4.2000\n",
            "Epoch 247/499\n",
            "----------\n",
            "Loss: 0.0230 Acc: 4.1467\n",
            "Epoch 248/499\n",
            "----------\n",
            "Loss: 0.0411 Acc: 3.8800\n",
            "Epoch 249/499\n",
            "----------\n",
            "Loss: 0.2107 Acc: 4.3467\n",
            "Epoch 250/499\n",
            "----------\n",
            "Loss: 0.0153 Acc: 4.0267\n",
            "Epoch 251/499\n",
            "----------\n",
            "Loss: 0.0383 Acc: 3.8133\n",
            "Epoch 252/499\n",
            "----------\n",
            "Loss: 0.0258 Acc: 4.4400\n",
            "Epoch 253/499\n",
            "----------\n",
            "Loss: 0.4157 Acc: 3.8000\n",
            "Epoch 254/499\n",
            "----------\n",
            "Loss: 0.0338 Acc: 4.2133\n",
            "Epoch 255/499\n",
            "----------\n",
            "Loss: 0.0199 Acc: 3.9467\n",
            "Epoch 256/499\n",
            "----------\n",
            "Loss: 0.0185 Acc: 3.9467\n",
            "Epoch 257/499\n",
            "----------\n",
            "Loss: 0.0326 Acc: 4.1867\n",
            "Epoch 258/499\n",
            "----------\n",
            "Loss: 0.0645 Acc: 4.2933\n",
            "Epoch 259/499\n",
            "----------\n",
            "Loss: 0.0667 Acc: 4.1600\n",
            "Epoch 260/499\n",
            "----------\n",
            "Loss: 0.1265 Acc: 4.2000\n",
            "Epoch 261/499\n",
            "----------\n",
            "Loss: 0.0483 Acc: 4.1867\n",
            "Epoch 262/499\n",
            "----------\n",
            "Loss: 0.0302 Acc: 4.1467\n",
            "Epoch 263/499\n",
            "----------\n",
            "Loss: 0.0390 Acc: 3.9733\n",
            "Epoch 264/499\n",
            "----------\n",
            "Loss: 0.0403 Acc: 4.0667\n",
            "Epoch 265/499\n",
            "----------\n",
            "Loss: 0.0770 Acc: 4.2533\n",
            "Epoch 266/499\n",
            "----------\n",
            "Loss: 0.0502 Acc: 4.0933\n",
            "Epoch 267/499\n",
            "----------\n",
            "Loss: 0.0785 Acc: 4.5067\n",
            "Epoch 268/499\n",
            "----------\n",
            "Loss: 0.0301 Acc: 3.8933\n",
            "Epoch 269/499\n",
            "----------\n",
            "Loss: 0.1435 Acc: 4.2133\n",
            "Epoch 270/499\n",
            "----------\n",
            "Loss: 0.0230 Acc: 4.0000\n",
            "Epoch 271/499\n",
            "----------\n",
            "Loss: 0.1728 Acc: 3.9467\n",
            "Epoch 272/499\n",
            "----------\n",
            "Loss: 0.0822 Acc: 3.8267\n",
            "Epoch 273/499\n",
            "----------\n",
            "Loss: 0.0125 Acc: 4.1200\n",
            "Epoch 274/499\n",
            "----------\n",
            "Loss: 0.0263 Acc: 3.8933\n",
            "Epoch 275/499\n",
            "----------\n",
            "Loss: 0.0378 Acc: 4.2267\n",
            "Epoch 276/499\n",
            "----------\n",
            "Loss: 0.0394 Acc: 4.0000\n",
            "Epoch 277/499\n",
            "----------\n",
            "Loss: 0.1242 Acc: 3.9200\n",
            "Epoch 278/499\n",
            "----------\n",
            "Loss: 0.0193 Acc: 4.4267\n",
            "Epoch 279/499\n",
            "----------\n",
            "Loss: 0.0194 Acc: 3.9467\n",
            "Epoch 280/499\n",
            "----------\n",
            "Loss: 0.0269 Acc: 4.0000\n",
            "Epoch 281/499\n",
            "----------\n",
            "Loss: 0.2935 Acc: 4.1467\n",
            "Epoch 282/499\n",
            "----------\n",
            "Loss: 0.0228 Acc: 4.0667\n",
            "Epoch 283/499\n",
            "----------\n",
            "Loss: 0.0186 Acc: 4.0800\n",
            "Epoch 284/499\n",
            "----------\n",
            "Loss: 0.1143 Acc: 4.1600\n",
            "Epoch 285/499\n",
            "----------\n",
            "Loss: 0.0134 Acc: 4.0400\n",
            "Epoch 286/499\n",
            "----------\n",
            "Loss: 0.1913 Acc: 4.2133\n",
            "Epoch 287/499\n",
            "----------\n",
            "Loss: 0.0353 Acc: 3.7467\n",
            "Epoch 288/499\n",
            "----------\n",
            "Loss: 0.0386 Acc: 4.2133\n",
            "Epoch 289/499\n",
            "----------\n",
            "Loss: 0.0146 Acc: 4.2800\n",
            "Epoch 290/499\n",
            "----------\n",
            "Loss: 0.0578 Acc: 3.9867\n",
            "Epoch 291/499\n",
            "----------\n",
            "Loss: 0.0589 Acc: 3.9733\n",
            "Epoch 292/499\n",
            "----------\n",
            "Loss: 0.0184 Acc: 3.7867\n",
            "Epoch 293/499\n",
            "----------\n",
            "Loss: 0.0168 Acc: 4.2000\n",
            "Epoch 294/499\n",
            "----------\n",
            "Loss: 0.2073 Acc: 3.8133\n",
            "Epoch 295/499\n",
            "----------\n",
            "Loss: 0.0412 Acc: 4.0000\n",
            "Epoch 296/499\n",
            "----------\n",
            "Loss: 0.0186 Acc: 4.3467\n",
            "Epoch 297/499\n",
            "----------\n",
            "Loss: 0.0493 Acc: 4.0533\n",
            "Epoch 298/499\n",
            "----------\n",
            "Loss: 0.0502 Acc: 4.2933\n",
            "Epoch 299/499\n",
            "----------\n",
            "Loss: 0.1042 Acc: 4.1867\n",
            "Epoch 300/499\n",
            "----------\n",
            "Loss: 0.0448 Acc: 4.0267\n",
            "Epoch 301/499\n",
            "----------\n",
            "Loss: 0.0809 Acc: 4.2933\n",
            "Epoch 302/499\n",
            "----------\n",
            "Loss: 0.0807 Acc: 4.2800\n",
            "Epoch 303/499\n",
            "----------\n",
            "Loss: 0.0472 Acc: 3.7333\n",
            "Epoch 304/499\n",
            "----------\n",
            "Loss: 0.0302 Acc: 4.2533\n",
            "Epoch 305/499\n",
            "----------\n",
            "Loss: 0.0263 Acc: 3.6667\n",
            "Epoch 306/499\n",
            "----------\n",
            "Loss: 0.0058 Acc: 3.7600\n",
            "Epoch 307/499\n",
            "----------\n",
            "Loss: 0.0090 Acc: 4.5067\n",
            "Epoch 308/499\n",
            "----------\n",
            "Loss: 0.3941 Acc: 4.3733\n",
            "Epoch 309/499\n",
            "----------\n",
            "Loss: 0.0792 Acc: 4.0000\n",
            "Epoch 310/499\n",
            "----------\n",
            "Loss: 0.0292 Acc: 4.2267\n",
            "Epoch 311/499\n",
            "----------\n",
            "Loss: 0.0542 Acc: 4.0267\n",
            "Epoch 312/499\n",
            "----------\n",
            "Loss: 0.0276 Acc: 4.2800\n",
            "Epoch 313/499\n",
            "----------\n",
            "Loss: 0.0435 Acc: 4.1200\n",
            "Epoch 314/499\n",
            "----------\n",
            "Loss: 0.0550 Acc: 4.2267\n",
            "Epoch 315/499\n",
            "----------\n",
            "Loss: 0.0369 Acc: 4.4267\n",
            "Epoch 316/499\n",
            "----------\n",
            "Loss: 0.0296 Acc: 4.1600\n",
            "Epoch 317/499\n",
            "----------\n",
            "Loss: 0.0299 Acc: 3.7467\n",
            "Epoch 318/499\n",
            "----------\n",
            "Loss: 0.0237 Acc: 4.2400\n",
            "Epoch 319/499\n",
            "----------\n",
            "Loss: 0.0606 Acc: 4.0533\n",
            "Epoch 320/499\n",
            "----------\n",
            "Loss: 0.0272 Acc: 3.6667\n",
            "Epoch 321/499\n",
            "----------\n",
            "Loss: 0.0174 Acc: 4.1733\n",
            "Epoch 322/499\n",
            "----------\n",
            "Loss: 0.0126 Acc: 4.1867\n",
            "Epoch 323/499\n",
            "----------\n",
            "Loss: 0.0178 Acc: 4.0267\n",
            "Epoch 324/499\n",
            "----------\n",
            "Loss: 0.0138 Acc: 3.9200\n",
            "Epoch 325/499\n",
            "----------\n",
            "Loss: 0.0831 Acc: 4.1867\n",
            "Epoch 326/499\n",
            "----------\n",
            "Loss: 0.0601 Acc: 4.4400\n",
            "Epoch 327/499\n",
            "----------\n",
            "Loss: 0.0174 Acc: 3.9467\n",
            "Epoch 328/499\n",
            "----------\n",
            "Loss: 0.0223 Acc: 3.8800\n",
            "Epoch 329/499\n",
            "----------\n",
            "Loss: 0.0779 Acc: 4.2533\n",
            "Epoch 330/499\n",
            "----------\n",
            "Loss: 0.0631 Acc: 4.0533\n",
            "Epoch 331/499\n",
            "----------\n",
            "Loss: 0.0318 Acc: 4.3733\n",
            "Epoch 332/499\n",
            "----------\n",
            "Loss: 0.0974 Acc: 4.2000\n",
            "Epoch 333/499\n",
            "----------\n",
            "Loss: 0.0120 Acc: 4.2133\n",
            "Epoch 334/499\n",
            "----------\n",
            "Loss: 0.0094 Acc: 4.0267\n",
            "Epoch 335/499\n",
            "----------\n",
            "Loss: 0.0084 Acc: 4.1467\n",
            "Epoch 336/499\n",
            "----------\n",
            "Loss: 0.4116 Acc: 4.2933\n",
            "Epoch 337/499\n",
            "----------\n",
            "Loss: 0.0196 Acc: 3.7867\n",
            "Epoch 338/499\n",
            "----------\n",
            "Loss: 0.0084 Acc: 3.9467\n",
            "Epoch 339/499\n",
            "----------\n",
            "Loss: 0.0201 Acc: 4.0800\n",
            "Epoch 340/499\n",
            "----------\n",
            "Loss: 0.1436 Acc: 4.1867\n",
            "Epoch 341/499\n",
            "----------\n",
            "Loss: 0.0136 Acc: 3.9600\n",
            "Epoch 342/499\n",
            "----------\n",
            "Loss: 0.1723 Acc: 4.2267\n",
            "Epoch 343/499\n",
            "----------\n",
            "Loss: 0.0118 Acc: 4.3067\n",
            "Epoch 344/499\n",
            "----------\n",
            "Loss: 0.0257 Acc: 4.0267\n",
            "Epoch 345/499\n",
            "----------\n",
            "Loss: 0.0520 Acc: 4.2267\n",
            "Epoch 346/499\n",
            "----------\n",
            "Loss: 0.0272 Acc: 4.1467\n",
            "Epoch 347/499\n",
            "----------\n",
            "Loss: 0.0312 Acc: 3.8933\n",
            "Epoch 348/499\n",
            "----------\n",
            "Loss: 0.1263 Acc: 4.0400\n",
            "Epoch 349/499\n",
            "----------\n",
            "Loss: 0.0751 Acc: 4.2000\n",
            "Epoch 350/499\n",
            "----------\n",
            "Loss: 0.0335 Acc: 4.0667\n",
            "Epoch 351/499\n",
            "----------\n",
            "Loss: 0.0375 Acc: 3.9733\n",
            "Epoch 352/499\n",
            "----------\n",
            "Loss: 0.0223 Acc: 4.0400\n",
            "Epoch 353/499\n",
            "----------\n",
            "Loss: 0.0666 Acc: 4.4800\n",
            "Epoch 354/499\n",
            "----------\n",
            "Loss: 0.0838 Acc: 3.6267\n",
            "Epoch 355/499\n",
            "----------\n",
            "Loss: 0.0461 Acc: 4.0667\n",
            "Epoch 356/499\n",
            "----------\n",
            "Loss: 0.0364 Acc: 4.1467\n",
            "Epoch 357/499\n",
            "----------\n",
            "Loss: 0.0579 Acc: 4.2800\n",
            "Epoch 358/499\n",
            "----------\n",
            "Loss: 0.0935 Acc: 4.2667\n",
            "Epoch 359/499\n",
            "----------\n",
            "Loss: 0.0098 Acc: 3.9200\n",
            "Epoch 360/499\n",
            "----------\n",
            "Loss: 0.0974 Acc: 4.1067\n",
            "Epoch 361/499\n",
            "----------\n",
            "Loss: 0.0212 Acc: 4.0000\n",
            "Epoch 362/499\n",
            "----------\n",
            "Loss: 0.1307 Acc: 4.0667\n",
            "Epoch 363/499\n",
            "----------\n",
            "Loss: 0.0898 Acc: 3.9867\n",
            "Epoch 364/499\n",
            "----------\n",
            "Loss: 0.0126 Acc: 4.2133\n",
            "Epoch 365/499\n",
            "----------\n",
            "Loss: 0.1196 Acc: 3.6933\n",
            "Epoch 366/499\n",
            "----------\n",
            "Loss: 0.1034 Acc: 3.6267\n",
            "Epoch 367/499\n",
            "----------\n",
            "Loss: 0.0316 Acc: 4.0267\n",
            "Epoch 368/499\n",
            "----------\n",
            "Loss: 0.0091 Acc: 4.0667\n",
            "Epoch 369/499\n",
            "----------\n",
            "Loss: 0.0442 Acc: 4.0667\n",
            "Epoch 370/499\n",
            "----------\n",
            "Loss: 0.0307 Acc: 3.8267\n",
            "Epoch 371/499\n",
            "----------\n",
            "Loss: 0.0905 Acc: 4.3067\n",
            "Epoch 372/499\n",
            "----------\n",
            "Loss: 0.0603 Acc: 4.0133\n",
            "Epoch 373/499\n",
            "----------\n",
            "Loss: 0.0191 Acc: 4.0667\n",
            "Epoch 374/499\n",
            "----------\n",
            "Loss: 0.0737 Acc: 4.2000\n",
            "Epoch 375/499\n",
            "----------\n",
            "Loss: 0.0256 Acc: 4.0800\n",
            "Epoch 376/499\n",
            "----------\n",
            "Loss: 0.0215 Acc: 4.0400\n",
            "Epoch 377/499\n",
            "----------\n",
            "Loss: 0.1187 Acc: 4.1067\n",
            "Epoch 378/499\n",
            "----------\n",
            "Loss: 0.0127 Acc: 4.0133\n",
            "Epoch 379/499\n",
            "----------\n",
            "Loss: 0.0270 Acc: 3.7867\n",
            "Epoch 380/499\n",
            "----------\n",
            "Loss: 0.0318 Acc: 3.9467\n",
            "Epoch 381/499\n",
            "----------\n",
            "Loss: 0.0909 Acc: 3.8267\n",
            "Epoch 382/499\n",
            "----------\n",
            "Loss: 0.0187 Acc: 4.0800\n",
            "Epoch 383/499\n",
            "----------\n",
            "Loss: 0.0684 Acc: 4.1333\n",
            "Epoch 384/499\n",
            "----------\n",
            "Loss: 0.0153 Acc: 4.0267\n",
            "Epoch 385/499\n",
            "----------\n",
            "Loss: 0.1663 Acc: 3.9600\n",
            "Epoch 386/499\n",
            "----------\n",
            "Loss: 0.0101 Acc: 3.7067\n",
            "Epoch 387/499\n",
            "----------\n",
            "Loss: 0.0884 Acc: 4.1600\n",
            "Epoch 388/499\n",
            "----------\n",
            "Loss: 0.0338 Acc: 4.0133\n",
            "Epoch 389/499\n",
            "----------\n",
            "Loss: 0.0331 Acc: 4.0267\n",
            "Epoch 390/499\n",
            "----------\n",
            "Loss: 0.0180 Acc: 4.1333\n",
            "Epoch 391/499\n",
            "----------\n",
            "Loss: 0.0284 Acc: 4.1467\n",
            "Epoch 392/499\n",
            "----------\n",
            "Loss: 0.1040 Acc: 4.4933\n",
            "Epoch 393/499\n",
            "----------\n",
            "Loss: 0.0311 Acc: 3.9333\n",
            "Epoch 394/499\n",
            "----------\n",
            "Loss: 0.0140 Acc: 4.1200\n",
            "Epoch 395/499\n",
            "----------\n",
            "Loss: 0.0924 Acc: 3.9733\n",
            "Epoch 396/499\n",
            "----------\n",
            "Loss: 0.0319 Acc: 4.0667\n",
            "Epoch 397/499\n",
            "----------\n",
            "Loss: 0.0444 Acc: 4.6533\n",
            "Epoch 398/499\n",
            "----------\n",
            "Loss: 0.0374 Acc: 4.0533\n",
            "Epoch 399/499\n",
            "----------\n",
            "Loss: 0.0115 Acc: 3.8800\n",
            "Epoch 400/499\n",
            "----------\n",
            "Loss: 0.0887 Acc: 4.1067\n",
            "Epoch 401/499\n",
            "----------\n",
            "Loss: 0.0124 Acc: 4.2667\n",
            "Epoch 402/499\n",
            "----------\n",
            "Loss: 0.0889 Acc: 4.1600\n",
            "Epoch 403/499\n",
            "----------\n",
            "Loss: 0.0357 Acc: 3.8000\n",
            "Epoch 404/499\n",
            "----------\n",
            "Loss: 0.0198 Acc: 4.1467\n",
            "Epoch 405/499\n",
            "----------\n",
            "Loss: 0.0349 Acc: 3.7200\n",
            "Epoch 406/499\n",
            "----------\n",
            "Loss: 0.0092 Acc: 4.1733\n",
            "Epoch 407/499\n",
            "----------\n",
            "Loss: 0.1802 Acc: 4.4933\n",
            "Epoch 408/499\n",
            "----------\n",
            "Loss: 0.0269 Acc: 4.2800\n",
            "Epoch 409/499\n",
            "----------\n",
            "Loss: 0.0323 Acc: 4.2933\n",
            "Epoch 410/499\n",
            "----------\n",
            "Loss: 0.0137 Acc: 4.4267\n",
            "Epoch 411/499\n",
            "----------\n",
            "Loss: 0.5328 Acc: 4.2133\n",
            "Epoch 412/499\n",
            "----------\n",
            "Loss: 0.0384 Acc: 4.3333\n",
            "Epoch 413/499\n",
            "----------\n",
            "Loss: 0.0215 Acc: 3.9333\n",
            "Epoch 414/499\n",
            "----------\n",
            "Loss: 0.0524 Acc: 4.3333\n",
            "Epoch 415/499\n",
            "----------\n",
            "Loss: 0.0230 Acc: 4.1333\n",
            "Epoch 416/499\n",
            "----------\n",
            "Loss: 0.0178 Acc: 4.1467\n",
            "Epoch 417/499\n",
            "----------\n",
            "Loss: 0.0127 Acc: 4.0800\n",
            "Epoch 418/499\n",
            "----------\n",
            "Loss: 0.0237 Acc: 4.0133\n",
            "Epoch 419/499\n",
            "----------\n",
            "Loss: 0.0106 Acc: 4.2133\n",
            "Epoch 420/499\n",
            "----------\n",
            "Loss: 0.0150 Acc: 4.0400\n",
            "Epoch 421/499\n",
            "----------\n",
            "Loss: 0.0292 Acc: 4.2533\n",
            "Epoch 422/499\n",
            "----------\n",
            "Loss: 0.0113 Acc: 4.1867\n",
            "Epoch 423/499\n",
            "----------\n",
            "Loss: 0.0112 Acc: 4.2800\n",
            "Epoch 424/499\n",
            "----------\n",
            "Loss: 0.0038 Acc: 4.0267\n",
            "Epoch 425/499\n",
            "----------\n",
            "Loss: 0.0437 Acc: 4.0800\n",
            "Epoch 426/499\n",
            "----------\n",
            "Loss: 0.0283 Acc: 4.2533\n",
            "Epoch 427/499\n",
            "----------\n",
            "Loss: 0.0158 Acc: 4.0133\n",
            "Epoch 428/499\n",
            "----------\n",
            "Loss: 0.4752 Acc: 3.8800\n",
            "Epoch 429/499\n",
            "----------\n",
            "Loss: 0.0811 Acc: 4.0000\n",
            "Epoch 430/499\n",
            "----------\n",
            "Loss: 0.0306 Acc: 4.2400\n",
            "Epoch 431/499\n",
            "----------\n",
            "Loss: 0.0338 Acc: 4.1333\n",
            "Epoch 432/499\n",
            "----------\n",
            "Loss: 0.0480 Acc: 4.0800\n",
            "Epoch 433/499\n",
            "----------\n",
            "Loss: 0.0088 Acc: 4.0667\n",
            "Epoch 434/499\n",
            "----------\n",
            "Loss: 0.0256 Acc: 3.6533\n",
            "Epoch 435/499\n",
            "----------\n",
            "Loss: 0.0250 Acc: 3.9733\n",
            "Epoch 436/499\n",
            "----------\n",
            "Loss: 0.0202 Acc: 4.0800\n",
            "Epoch 437/499\n",
            "----------\n",
            "Loss: 0.0356 Acc: 4.0400\n",
            "Epoch 438/499\n",
            "----------\n",
            "Loss: 0.0575 Acc: 3.8133\n",
            "Epoch 439/499\n",
            "----------\n",
            "Loss: 0.0251 Acc: 3.9600\n",
            "Epoch 440/499\n",
            "----------\n",
            "Loss: 0.0291 Acc: 4.4267\n",
            "Epoch 441/499\n",
            "----------\n",
            "Loss: 0.0345 Acc: 4.2000\n",
            "Epoch 442/499\n",
            "----------\n",
            "Loss: 0.0102 Acc: 4.0800\n",
            "Epoch 443/499\n",
            "----------\n",
            "Loss: 0.0108 Acc: 4.4667\n",
            "Epoch 444/499\n",
            "----------\n",
            "Loss: 0.0174 Acc: 4.1867\n",
            "Epoch 445/499\n",
            "----------\n",
            "Loss: 0.0143 Acc: 4.1867\n",
            "Epoch 446/499\n",
            "----------\n",
            "Loss: 0.0513 Acc: 4.4133\n",
            "Epoch 447/499\n",
            "----------\n",
            "Loss: 0.0240 Acc: 4.2133\n",
            "Epoch 448/499\n",
            "----------\n",
            "Loss: 0.0128 Acc: 4.1733\n",
            "Epoch 449/499\n",
            "----------\n",
            "Loss: 0.0092 Acc: 4.2667\n",
            "Epoch 450/499\n",
            "----------\n",
            "Loss: 0.0241 Acc: 3.9200\n",
            "Epoch 451/499\n",
            "----------\n",
            "Loss: 0.2766 Acc: 4.1067\n",
            "Epoch 452/499\n",
            "----------\n",
            "Loss: 0.0685 Acc: 4.3333\n",
            "Epoch 453/499\n",
            "----------\n",
            "Loss: 0.0324 Acc: 4.1200\n",
            "Epoch 454/499\n",
            "----------\n",
            "Loss: 0.0276 Acc: 3.8133\n",
            "Epoch 455/499\n",
            "----------\n",
            "Loss: 0.0185 Acc: 4.2933\n",
            "Epoch 456/499\n",
            "----------\n",
            "Loss: 0.0150 Acc: 4.0400\n",
            "Epoch 457/499\n",
            "----------\n",
            "Loss: 0.2941 Acc: 3.9067\n",
            "Epoch 458/499\n",
            "----------\n",
            "Loss: 0.0280 Acc: 4.1067\n",
            "Epoch 459/499\n",
            "----------\n",
            "Loss: 0.0170 Acc: 3.9200\n",
            "Epoch 460/499\n",
            "----------\n",
            "Loss: 0.0229 Acc: 4.3600\n",
            "Epoch 461/499\n",
            "----------\n",
            "Loss: 0.0156 Acc: 3.9867\n",
            "Epoch 462/499\n",
            "----------\n",
            "Loss: 0.0182 Acc: 3.7600\n",
            "Epoch 463/499\n",
            "----------\n",
            "Loss: 0.2951 Acc: 3.8000\n",
            "Epoch 464/499\n",
            "----------\n",
            "Loss: 0.0779 Acc: 3.7067\n",
            "Epoch 465/499\n",
            "----------\n",
            "Loss: 0.0175 Acc: 3.9467\n",
            "Epoch 466/499\n",
            "----------\n",
            "Loss: 0.0032 Acc: 4.1733\n",
            "Epoch 467/499\n",
            "----------\n",
            "Loss: 0.0542 Acc: 4.2667\n",
            "Epoch 468/499\n",
            "----------\n",
            "Loss: 0.0868 Acc: 3.8800\n",
            "Epoch 469/499\n",
            "----------\n",
            "Loss: 0.0190 Acc: 4.1067\n",
            "Epoch 470/499\n",
            "----------\n",
            "Loss: 0.0416 Acc: 4.4267\n",
            "Epoch 471/499\n",
            "----------\n",
            "Loss: 0.0230 Acc: 3.7467\n",
            "Epoch 472/499\n",
            "----------\n",
            "Loss: 0.0268 Acc: 3.9067\n",
            "Epoch 473/499\n",
            "----------\n",
            "Loss: 0.0661 Acc: 3.9200\n",
            "Epoch 474/499\n",
            "----------\n",
            "Loss: 0.0346 Acc: 4.1067\n",
            "Epoch 475/499\n",
            "----------\n",
            "Loss: 0.0307 Acc: 3.9067\n",
            "Epoch 476/499\n",
            "----------\n",
            "Loss: 0.0382 Acc: 4.4267\n",
            "Epoch 477/499\n",
            "----------\n",
            "Loss: 0.0091 Acc: 3.6533\n",
            "Epoch 478/499\n",
            "----------\n",
            "Loss: 0.0190 Acc: 4.0267\n",
            "Epoch 479/499\n",
            "----------\n",
            "Loss: 0.1542 Acc: 4.0800\n",
            "Epoch 480/499\n",
            "----------\n",
            "Loss: 0.0224 Acc: 4.1733\n",
            "Epoch 481/499\n",
            "----------\n",
            "Loss: 0.0407 Acc: 4.0533\n",
            "Epoch 482/499\n",
            "----------\n",
            "Loss: 0.0651 Acc: 4.0400\n",
            "Epoch 483/499\n",
            "----------\n",
            "Loss: 0.0638 Acc: 3.7467\n",
            "Epoch 484/499\n",
            "----------\n",
            "Loss: 0.0563 Acc: 4.3200\n",
            "Epoch 485/499\n",
            "----------\n",
            "Loss: 0.0607 Acc: 4.1467\n",
            "Epoch 486/499\n",
            "----------\n",
            "Loss: 0.0647 Acc: 4.0667\n",
            "Epoch 487/499\n",
            "----------\n",
            "Loss: 0.0560 Acc: 4.1867\n",
            "Epoch 488/499\n",
            "----------\n",
            "Loss: 0.0107 Acc: 4.3067\n",
            "Epoch 489/499\n",
            "----------\n",
            "Loss: 0.0497 Acc: 3.8933\n",
            "Epoch 490/499\n",
            "----------\n",
            "Loss: 0.0305 Acc: 3.9333\n",
            "Epoch 491/499\n",
            "----------\n",
            "Loss: 0.0081 Acc: 3.9867\n",
            "Epoch 492/499\n",
            "----------\n",
            "Loss: 0.0434 Acc: 4.3067\n",
            "Epoch 493/499\n",
            "----------\n",
            "Loss: 0.0286 Acc: 3.9867\n",
            "Epoch 494/499\n",
            "----------\n",
            "Loss: 0.0162 Acc: 4.0800\n",
            "Epoch 495/499\n",
            "----------\n",
            "Loss: 0.1118 Acc: 3.9467\n",
            "Epoch 496/499\n",
            "----------\n",
            "Loss: 0.0113 Acc: 4.3067\n",
            "Epoch 497/499\n",
            "----------\n",
            "Loss: 0.0211 Acc: 4.0933\n",
            "Epoch 498/499\n",
            "----------\n",
            "Loss: 0.2719 Acc: 4.2000\n",
            "Epoch 499/499\n",
            "----------\n",
            "Loss: 0.0082 Acc: 4.1067\n",
            "Best val Acc: 4.666667\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yON3t8WXhdq"
      },
      "source": [
        "#Resultados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EXD9iI_6yHD"
      },
      "source": [
        "def contarCorrectas(net,batch,labels,func=None):\n",
        "    '''Dado un batch y sus etiquetas, cuenta el numero de respuestas\n",
        "    correctas de una red, el parametro func aplica una modificacion al \n",
        "    tensor que contiene los datos'''\n",
        "    \n",
        "    if(func!=None):\n",
        "        batch=func(batch)\n",
        "        salidas=net(batch)\n",
        "    else:\n",
        "        salidas=net(batch)\n",
        "    respuestas=salidas.max(dim=1)[1]\n",
        "    cantidadCorrectas=(respuestas==torch.argmax(labels)).sum()\n",
        "    return cantidadCorrectas\n",
        "\n",
        "def calcularPrecisionGlobal(net,data_loader,batch_size,func=None,cuda=False):\n",
        "    '''Calcula la precision de una red dado un data_loader,\n",
        "    recive una funcion que transforma los datos en caso de ser necesario'''\n",
        "    correctas=0\n",
        "    for (images,labels) in data_loader:\n",
        "        if(cuda and torch.cuda.is_available()):\n",
        "            images=images.cuda()\n",
        "            labels=labels.cuda()\n",
        "        correctas+=contarCorrectas(net,images,labels,func)        \n",
        "    correctas=correctas.data.tolist()\n",
        "    return (100*correctas)/(len(data_loader)*batch_size) "
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPth9yWaXgCP",
        "outputId": "94daa886-118b-42ad-ed8b-aaf0be08f88e"
      },
      "source": [
        "prec_train =calcularPrecisionGlobal(music_net,train_dataloader,10,cuda=True)\n",
        "prec_val   =calcularPrecisionGlobal(music_net,test_dataloader,10,cuda=True)\n",
        "print(\"Precision en conjunto de entrenamiento: %.4f%%\"%(prec_train))\n",
        "print(\"Precision en conjunto de validacion: %.4f%%\"%(prec_val))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision en conjunto de entrenamiento: 43.4667%\n",
            "Precision en conjunto de validacion: 34.2105%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9qJ_EnfozBP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}